# Test config file for Blip
module:
  module_name:  'corenet_v1'
  module_type:  ['ml']
  module_mode:  ['training']
  gpu:          True
  gpu_device:   0
  verbose:      False

dataset:
  dataset_folder: "/home/ncarrara/physics/cmssm/"
  dataset_files:  []
  dataset_type:   'cmssm'
  normalized:     true
  # ---- variables ----
  # Here we specify the location of the GUT variables in the files
  # and the weak values associated to the trial points.
  gut_test: [0,5]
  gut_true: [7,12]
  weak_test: [5,7]
  labels: []
  # ---- normalization ----
  # Normalization parameters for positions and features.  Typically we want to normalize
  # input features depending on the type of task, or to unbias certain simulation parameters,
  # e.g. by normalizaing ADC over an event sample we remove the overall scale of the detector
  # response.
  position_normalization:   []
  features_normalization:   []

loader:
  loader_type:      ""
  batch_size:       518
  test_split:       0.1
  test_seed:        0
  validation_split: 0.3
  validation_seed:  0
  num_workers:      4

training:
  iterations:   1
  epochs:       10
  checkpoint:   10
  grad_norm:    true
  progress_bar: "all"        # train, validation, test, all
  rewrite_bar:      False # wether to leave bars after each epoch
  save_predictions: False  # wether to save network outputs in original file
  no_timing:    False     # wether to keep timing/memory info in callback
  skip_metrics: True
  seed:         0

model:
  # uncomment the line below and specify the model to load from a checkpoint.
  # load_model:   ".checkpoints/checkpoint_200.ckpt"
  CORENet:
    mix_gradients: true
    chuncc: true
    input_dimension:      5
    # encoder parameters
    encoder_dimensions:   [25, 50, 100, 50, 25]
    encoder_activation:   'leaky_relu'
    encoder_activation_params:    {'negative_slope': 0.02}
    encoder_normalization: 'batch_norm'
    # desired dimension of the latent space
    latent_dimension:     5
    # decoder parameters
    decoder_dimensions:   [25, 50, 100, 50, 25]
    decoder_activation:   'leaky_relu'
    decoder_activation_params:    {'negative_slope': 0.02}
    decoder_normalization: 'batch_norm'
    # output activation
    output_activation:    'linear'
    output_activation_params:     {}
    # core parameters
    core_input_dimension: 7
    core_dimensions:      [25, 50, 100, 50, 25]
    core_activation:   'leaky_relu'
    core_activation_params:    {'negative_slope': 0.02}
    core_normalization: 'batch_norm'

criterion:
  alpha: 1.5
  L2GUTLoss:
    alpha: 1
  LatentWassersteinLoss:
    alpha: 1.0
    latent_variables: [0, 1, 2, 3, 4]
    num_projections: 1000
  LatentBinaryLoss:
    alpha: 1.0

metrics:

callbacks:

optimizer:
  optimizer_type: "Adam"
  learning_rate:  0.01
  betas:          [0.9, 0.999]
  epsilon:        1e-08
  weight_decay:   0.001
  momentum:       0.9