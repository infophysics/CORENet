# Test config file for Blip
module:
  module_name:  'blank_config'
  module_type:  ['ml']
  module_mode:  ['training']
  gpu:          True
  gpu_device:   0
  verbose:      False

dataset:
  dataset_folder: ""
  dataset_files:  []
  normalized:     False
  # ---- variables ----
  # Here we specify the location of the GUT variables in the files
  # and the weak values associated to the trial points.
  gut_test: []
  gut_true: []
  weak_test: []
  labels: []
  # ---- normalization ----
  # Normalization parameters for positions and features.  Typically we want to normalize
  # input features depending on the type of task, or to unbias certain simulation parameters,
  # e.g. by normalizaing ADC over an event sample we remove the overall scale of the detector
  # response.
  position_normalization:   []
  features_normalization:   []

loader:
  loader_type:      ""
  batch_size:       1
  test_split:       0.0
  test_seed:        0
  validation_split: 0.0
  validation_seed:  0
  num_workers:      1

training:
  iterations:   1
  epochs:       1
  checkpoint:   1
  progress_bar: "all"        # train, validation, test, all
  rewrite_bar:      False # wether to leave bars after each epoch
  save_predictions: True  # wether to save network outputs in original file
  no_timing:    False     # wether to keep timing/memory info in callback
  skip_metrics: False
  seed:         0

model:
  # uncomment the line below and specify the model to load from a checkpoint.
  # load_model:   ".checkpoints/checkpoint_200.ckpt"
  CORENet:
    input_dimension:      5
    # encoder parameters
    encoder_dimensions:   [10, 25, 50, 25, 10]
    encoder_activation:   'leaky_relu'
    encoder_activation_params:    {'negative_slope': 0.02}
    encoder_normalization: 'bias'
    # desired dimension of the latent space
    latent_dimension:     5
    # decoder parameters
    decoder_dimensions:   [10, 25, 50, 25, 10]
    decoder_activation:   'leaky_relu'
    decoder_activation_params:    {'negative_slope': 0.02}
    decoder_normalization: 'bias'
    # output activation
    output_activation:    'linear'
    output_activation_params:     {}
    # core parameters
    core_input_dimension: 4
    core_dimensions:      [10, 25, 50, 25, 10]
    core_activation:   'leaky_relu'
    core_activation_params:    {'negative_slope': 0.02}
    core_normalization: 'bias'

criterion:
    L2GUTLoss:
      alpha: 1

metrics:

callbacks:

optimizer:
  optimizer_type: "Adam"
  learning_rate:  0.01
  betas:          [0.9, 0.999]
  epsilon:        1e-08
  weight_decay:   0.001
  momentum:       0.9